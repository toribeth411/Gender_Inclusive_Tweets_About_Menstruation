{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "793d45fd",
   "metadata": {},
   "source": [
    "# 1. Creating New Twitter Dataset\n",
    "\n",
    "**Author:** Tori Stiegman   \n",
    "**Project:** Gender-Inclusive Language in Tweets about Menstruation   \n",
    "**Date turned in:** Dec 19, 2022\n",
    "**Updated:** Feb 28, 2023\n",
    "\n",
    "**About this notebook:** In this notebook, I will extract tweets and their data from documents of output from the Twitter API. I will then export this data to a new CSV file. \n",
    "\n",
    "**Table of Contents:**\n",
    "1. [Load in Raw Data](#sec1)\n",
    "2. [Create dataframe](#sec2)\n",
    "3. [Drop Duplicates](#sec3)\n",
    "4. [Create CVS](#sec4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551483de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import advertools as adv\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# get rid of warnings pls\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731bf4e",
   "metadata": {},
   "source": [
    "<a name=\"sec1\"></a>\n",
    "## Load in the Raw Data\n",
    "\n",
    "From data collected from Eni in the folder `twitter-dataset-1`. It contains 325 files, with roughly 1000 tweets/file\n",
    "\n",
    "**Keywords:** \n",
    "- menstrual cycle\n",
    "- tracking your period\n",
    "- period tracking\n",
    "- track your menstrual cycle\n",
    "- period tracker\n",
    "- menstruation \n",
    "\n",
    "**Timeline of Tweets:**\n",
    "Start: 11/10/20\n",
    "End: 11/10/22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4bcd31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweetListFull = []\n",
    "\n",
    "for i in range(327):\n",
    "    file_name = \"results_\" + str(i) + \".json\"\n",
    "    with open(file_name, 'r') as inFile:\n",
    "        tweetListFull.append(json.load(inFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed945b4f",
   "metadata": {},
   "source": [
    "<a name=\"sec2\"></a>\n",
    "## Create Dataframe\n",
    "\n",
    "Create a dataframe, `dfTweetsFull`, by looping through each file and pulling out specific elements of the tweets:\n",
    "- tweet id\n",
    "- author id\n",
    "- text of the tweet\n",
    "- date of the tweet\n",
    "- retweet_count\n",
    "- like_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a526420",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetList = []\n",
    "\n",
    "for h in range(len(tweetListFull)):\n",
    "    \n",
    "    document = tweetListFull[h]\n",
    "    \n",
    "    for i in range(len(document)):\n",
    "\n",
    "        tweets = tweetListFull[h][i]['tweets']\n",
    "\n",
    "        for j in range(len(tweets)):\n",
    "\n",
    "            singleTweetDict = {}\n",
    "\n",
    "            # extract the information and add to single tweet dictionary\n",
    "            singleTweetDict['tweet_id'] = tweetListFull[h][i]['tweets'][j]['id']\n",
    "            singleTweetDict['author_id'] = tweetListFull[h][i]['tweets'][j]['author_id']\n",
    "            singleTweetDict['text'] = tweetListFull[h][i]['tweets'][j]['text']\n",
    "            singleTweetDict['date'] = tweetListFull[h][i]['tweets'][j]['created_at'].split(' ')[0]\n",
    "            singleTweetDict['retweet_count'] = tweetListFull[h][i]['tweets'][j]['public_metrics']['retweet_count']\n",
    "            singleTweetDict['reply_count'] = tweetListFull[h][i]['tweets'][j]['public_metrics']['reply_count']\n",
    "            singleTweetDict['like_count'] = tweetListFull[h][i]['tweets'][j]['public_metrics']['like_count']\n",
    "\n",
    "            # add the dictionary to tweet list\n",
    "            tweetList.append(singleTweetDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2786a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweetsFull = pd.DataFrame(tweetList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "418c81c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309987, 7)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTweetsFull.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184e622",
   "metadata": {},
   "source": [
    "<a name=\"sec3\"></a>\n",
    "## Drop Tweets with duplicate text\n",
    "\n",
    "Drop tweets that are duplicated. Only the first instance will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f29c3b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweetsNoDupe = dfTweetsFull.drop_duplicates(subset = \"text\",\n",
    "                     keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6975763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301151, 7)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTweetsNoDupe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec95b75",
   "metadata": {},
   "source": [
    "<a name=\"sec4\"></a>\n",
    "## Create CSV\n",
    "\n",
    "Create a CSV, `period_tweets.csv`, containing all of the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b0a72203",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweetsNoDupe.to_csv('period_tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
