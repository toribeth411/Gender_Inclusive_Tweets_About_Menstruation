{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d76fe80",
   "metadata": {},
   "source": [
    "# 3. Cleaning My Period Tracking Data\n",
    "\n",
    "\n",
    "**Author:** Tori Stiegman   \n",
    "**Project:** Gender-Inclusive Language in Tweets about Menstruation   \n",
    "**Date turned in:** Dec 19, 2022\n",
    "\n",
    "**About this notebook:** In this notebook I will go thorugh the process of cleaning and preparing the Twitter data I extracted in an eariler notebook. \n",
    "\n",
    "\n",
    "**Table of Contents**\n",
    "1. [Load Data](#data)\n",
    "2. [Remove Emojis](#emoji)\n",
    "3. [Make All Letters Lowercase](#lower)\n",
    "4. [Remove Hashtags and Mentions)](#hashtag)\n",
    "5. [Remove links](#link)\n",
    "6. [Remove punctuations and non-alphanumeric characters](#punct)\n",
    "7. [Tokenization](#token)\n",
    "8. [Stemming](#stem)\n",
    "9. [Export to CSV](#csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "163becde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import advertools as adv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# get rid of warnings pls\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e36102",
   "metadata": {},
   "source": [
    "<a name=\"data\"></a>\n",
    "## Load Data\n",
    "\n",
    "Here I will load in my data, which includes:\n",
    "- `noTrain_fullTweets.csv`: CSV with full twitter data excluding the training and testing sets\n",
    "- `2023_training_labeled.csv`: CSV with labeled training set\n",
    "- `2023_test_labeled.csv`: CSV with labeled testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "477d7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can see the whole tweet text\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca891b0",
   "metadata": {},
   "source": [
    "Load in the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e66b335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFull = pd.read_csv('noTrainTest_fullTweets.csv')\n",
    "dfFull.head()\n",
    "\n",
    "# create a duplicate dataset that we can clean\n",
    "dfFullClean = dfFull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca11d3",
   "metadata": {},
   "source": [
    "Load in training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7717faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTraining = pd.read_csv(\"2023_training_labeled.csv\")\n",
    "\n",
    "trainClean = dfTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83416d31",
   "metadata": {},
   "source": [
    "Load in testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "742f9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = pd.read_csv(\"2023_test_labeled.csv\")\n",
    "\n",
    "testClean = dfTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1011411d",
   "metadata": {},
   "source": [
    "Make a new column, `text_clean`, that I will clean throughout the rest of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7ab94b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "makeString = lambda text: str(text)\n",
    "\n",
    "dfFullClean['text_clean'] = dfFull['text'].apply(makeString)\n",
    "trainClean['text_clean'] = trainClean['text'].apply(makeString)\n",
    "testClean['text_clean'] = testClean['text'].apply(makeString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56645f56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dfFullClean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d58d97",
   "metadata": {},
   "source": [
    "<a name=\"emoji\"></a>\n",
    "## Remove Emojis\n",
    "\n",
    "Remove emojis from tweets and replace with a blank space, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69db8c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                        u\"\\U00002702-\\U000027B0\"\n",
    "                        u\"\\U000024C2-\\U0001F251\"\n",
    "                        u\"\\U0001f926-\\U0001f937\"\n",
    "                        u\"\\U0001F1F2\"\n",
    "                        u\"\\U0001F1F4\"\n",
    "                        u\"\\U0001F620\"\n",
    "                        u\"\\u200d\"\n",
    "                        u\"\\u2640-\\u2642\"\n",
    "                        u\"\\u2600-\\u2B55\"\n",
    "                        u\"\\u23cf\"\n",
    "                        u\"\\u23e9\"\n",
    "                        u\"\\u231a\"\n",
    "                        u\"\\ufe0f\"  # dingbats\n",
    "                        u\"\\u3030\"\n",
    "                        u\"\\U00002500-\\U00002BEF\"  # Chinese char\n",
    "                        u\"\\U00010000-\\U0010ffff\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "emoji = lambda text: re.sub(emoji_pattern,\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bfb968e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFullClean['text_clean'] = dfFull['text_clean'].apply(emoji)\n",
    "trainClean['text_clean'] = trainClean['text_clean'].apply(emoji)\n",
    "testClean['text_clean'] = testClean['text_clean'].apply(emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c9920",
   "metadata": {},
   "source": [
    "<a name=\"lower\"></a>\n",
    "## Make all Letters Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06e78c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "lower = lambda text: text.lower()\n",
    "\n",
    "dfFullClean['text_clean'] = dfFullClean['text_clean'].apply(lower)\n",
    "trainClean['text_clean'] = trainClean['text_clean'].apply(lower)\n",
    "testClean['text_clean'] = testClean['text_clean'].apply(lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd4166",
   "metadata": {},
   "source": [
    "<a name=\"hashtag\"></a>\n",
    "## Remove Hashtags and Mentions\n",
    "\n",
    "Remove hashtags and Twitter user mentions from tweets and replace with a blank space, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff48efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "mentions = lambda text: re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "hashtags = lambda text: re.sub(\"#[A-Za-z0-9_]+\",\"\", text)\n",
    "\n",
    "# apply the helper functions\n",
    "dfFullClean['text_clean'] = dfFullClean['text_clean'].apply(mentions)\n",
    "dfFullClean['text_clean'] = dfFullClean['text_clean'].apply(hashtags)\n",
    "\n",
    "trainClean['text_clean'] = trainClean['text_clean'].apply(mentions)\n",
    "trainClean['text_clean'] = trainClean['text_clean'].apply(hashtags)\n",
    "\n",
    "testClean['text_clean'] = testClean['text_clean'].apply(mentions)\n",
    "testClean['text_clean'] = testClean['text_clean'].apply(hashtags)\n",
    "\n",
    "dfFullClean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f332ec",
   "metadata": {},
   "source": [
    "<a name=\"link\"></a>\n",
    "## Remove links\n",
    "\n",
    "Remove hyperlinks from tweets and replace with a blank space, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "469881aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "http = lambda text: re.sub(r\"http\\S+\", \"\", text)\n",
    "www = lambda text: re.sub(r\"www.\\S+\", \"\", text)\n",
    "\n",
    "# apply helper functions\n",
    "dfFullClean['text_clean'] = dfFullClean['text_clean'].apply(http)\n",
    "dfFullClean['text_clean'] = dfFullClean['text_clean'].apply(www)\n",
    "\n",
    "trainClean['text_clean'] = trainClean['text_clean'].apply(http)\n",
    "trainClean['text_clean'] = trainClean['text_clean'].apply(www)\n",
    "\n",
    "testClean['text_clean'] = testClean['text_clean'].apply(http)\n",
    "testClean['text_clean'] = testClean['text_clean'].apply(www)\n",
    "\n",
    "dfFullClean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ba8d1",
   "metadata": {},
   "source": [
    "<a name=\"punct\"></a>\n",
    "## Remove punctuations and non-alphanumeric characters\n",
    "\n",
    "Remove punctuation and non-alphanumeric characters from tweets and replace with a blank space, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c59d12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "one = lambda text: re.sub('[()!?&]', ' ', text) ## May not want to remove exclamation points...\n",
    "two = lambda text: re.sub('\\[.*?\\-]',' ', text)\n",
    "nonAlpha = lambda text: re.sub(\"[^a-z0-9]\",\" \", text)\n",
    "\n",
    "# apply helper functions\n",
    "dfFullClean['text_clean'] = dfFullClean['text_clean'].apply(one)\n",
    "dfFullClean['text_clean'] = dfFullClean['text_clean'].apply(two)\n",
    "dfFullClean['text_clean'] = dfFullClean['text_clean'].apply(nonAlpha)\n",
    "\n",
    "trainClean['text_clean'] = trainClean['text_clean'].apply(one)\n",
    "trainClean['text_clean'] = trainClean['text_clean'].apply(two)\n",
    "trainClean['text_clean'] = trainClean['text_clean'].apply(nonAlpha)\n",
    "\n",
    "testClean['text_clean'] = testClean['text_clean'].apply(one)\n",
    "testClean['text_clean'] = testClean['text_clean'].apply(two)\n",
    "testClean['text_clean'] = testClean['text_clean'].apply(nonAlpha)\n",
    "\n",
    "# dfFullClean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5e880",
   "metadata": {},
   "source": [
    "<a name=\"token\"></a>\n",
    "## Tokenization\n",
    "\n",
    "Turn each word in the sentence into a \"token.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d115994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "370171d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96cdb681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "tt = TweetTokenizer()\n",
    "token = lambda text: tt.tokenize(text)\n",
    "\n",
    "# apply function\n",
    "dfFullClean['text_clean'] = dfFullClean['text_clean'].apply(makeString).apply(token)\n",
    "trainClean['text_clean'] = trainClean['text_clean'].apply(makeString).apply(token)\n",
    "testClean['text_clean'] = testClean['text_clean'].apply(makeString).apply(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb6db51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainClean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b349bc",
   "metadata": {},
   "source": [
    "<a name=\"stem\"></a>\n",
    "## Stemming\n",
    "\n",
    "Reduce each word to its stem that affixes to suffixes and prefixes or to the roots of words known as \"lemmas.\"\n",
    "This will be very helpful when feeding the tweets into a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac3c1e",
   "metadata": {},
   "source": [
    "### Specify stop words\n",
    "\n",
    "Specify stop words, or short words, to get rid of while stemming. \n",
    "\n",
    "I will keep these stop words since they are related to my query: \n",
    "\"he\", \"him\", \"his\", \"himself\", \"she\", \"she\", \"she's\", \"her\", \"hers\", \"herself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dee4ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')\n",
    "stop_words_og = stop_words\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "pronouns = [\"he\", \"him\", \"his\", \"himself\", \"she\", \"she\", \"she's\", \"her\", \"hers\", \"herself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\"]\n",
    "\n",
    "for word in pronouns:\n",
    "    if word in stop_words:\n",
    "        stop_words.remove(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af782e",
   "metadata": {},
   "source": [
    "### Stem each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1891433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# helper function\n",
    "stem = lambda text: [stemmer.stem(word) for word in text if (word not in stop_words)]\n",
    "\n",
    "# apply function\n",
    "dfFullClean['text_clean'] = dfFullClean['text_clean'].apply(stem)\n",
    "trainClean['text_clean'] = trainClean['text_clean'].apply(stem)\n",
    "testClean['text_clean'] = testClean['text_clean'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "892982c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainClean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ce3f4",
   "metadata": {},
   "source": [
    "### Join stemmed list together\n",
    "\n",
    "Join list of stemmed words together to create a stemmed phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "db5f4660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "join = lambda lst: ' '.join(lst)\n",
    "\n",
    "# apply helper function\n",
    "dfFullClean['text_clean'] = dfFullClean['text_clean'].apply(join)\n",
    "trainClean['text_clean'] = trainClean['text_clean'].apply(join)\n",
    "testClean['text_clean'] = testClean['text_clean'].apply(join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b452b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfFullClean.head()\n",
    "# trainClean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94261294",
   "metadata": {},
   "source": [
    "<a name=\"csv\"></a>\n",
    "## Export to CSV\n",
    "\n",
    "Create six files:\n",
    "\n",
    "The first three will be used for my first Naive Bayes Model:\n",
    "1. `fullTwitter_clean.csv`\n",
    "2. `train_clean.csv`\n",
    "3. `test_clean.csv`\n",
    "\n",
    "The next three will be used for the new Naive Bayes model and the classification tree:          \n",
    "4. `fullTwitter_clean_extras.csv`                               \n",
    "5. `train_clean_extras.csv`                                    \n",
    "6. `test_clean_extras.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "04619595",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFullText = dfFullClean.loc[:,[\"text\", \"tweet_id\", \"text_clean\"]]\n",
    "dfFullText.to_csv('fullTwitter_clean.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ec25d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainClean_text = trainClean.loc[:,[\"text\", \"label\", \"tweet_id\", \"text_clean\"]]\n",
    "trainClean_text.to_csv('train_clean.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "87f75814",
   "metadata": {},
   "outputs": [],
   "source": [
    "testClean_text = testClean.loc[:,[\"text\", \"label\", \"tweet_id\", \"text_clean\"]]\n",
    "testClean_text.to_csv('test_clean.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea9b2c",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9c249052",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFullText_extras = dfFullClean.loc[:,[\"text\", \"tweet_id\", \"text_clean\", \"date\", \"like_count\"]]\n",
    "dfFullText_extras.to_csv('fullTwitter_clean_extras.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1839b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainClean_text_extras = trainClean.loc[:,[\"text\", \"label\", \"tweet_id\", \"text_clean\", \"date\", \"like_count\"]]\n",
    "trainClean_text_extras.to_csv('train_clean_extras.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e196b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "testClean_text_extras = testClean.loc[:,[\"text\", \"label\", \"tweet_id\", \"text_clean\", \"date\", \"like_count\"]]\n",
    "testClean_text_extras.to_csv('test_clean_extras.csv', index = False, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
